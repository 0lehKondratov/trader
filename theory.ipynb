{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The input dataset (taken from\n",
    "[this](https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories)\n",
    "Kaggle challenge) consists of approximately $8000$ CSV files, each of which\n",
    "contains historical price records for a stock symbol from 1970\n",
    "to 2018. Each record consists of the following columns:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "date, high, low, open, close, volume, adjclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high, low, open, and close prices are historical prices as\n",
    "recorded by the market on that day. The `adjclose` column provides an\n",
    "adjusted close price that accounts for events like splits, which\n",
    "retroactively affect historical price data. For example, if a company doubles its number of existing shares, the\n",
    "price per share will be reduced by half. The adjusted close price\n",
    "accounts for these changes, providing a shared scale on which to\n",
    "measure closing price over the history of the stock.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Several filtering techniques can be applied to the input dataset in\n",
    "order to produce higher quality training examples. One such technique\n",
    "is the removal records older than a parameterized year.\n",
    "Since the dataset contains records dating back to 1970, the\n",
    "ability to constrain to more recent records may prove useful in\n",
    "capturing market behavior under modern trading styles.\n",
    "\n",
    "Another filtering technique is to remove penny stocks, which tend to exhibit greater\n",
    "volatility due to their low price per share. This is accomplished\n",
    "through the use of a parameterized closing price threshold. If the\n",
    "average closing price of a stock over the constrained date window is\n",
    "below the parameterized threshold, that stock will be excluded\n",
    "entirely from the training example set.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "A positional encoding strategy was used to pass date information from\n",
    "each record to the neural network in a numerically stable form. The underlying assumption\n",
    "is that stocks may behave differently at different times of year (e.g.\n",
    "during quarterly earnings reports, end of year, etc.), and thus this\n",
    "information could prove useful to the network. The following function\n",
    "was used to calculate a positional encoding based on the day of year:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin\\left(\\frac{\\pi x}{365}\\right)\n",
    "$$\n",
    "\n",
    "This function transforms a day of year on the interval $[1, 365]$ into a real number on a\n",
    "continuous interval from $(0, 1]$. Under this transformation, day\n",
    "$365$ and day $1$ will have similar values, capturing the idea that\n",
    "December 31st and January 1st are close to each other. It is worth\n",
    "noting that each point under this transformation will be produced by\n",
    "two points in the original \"day of year\" space. This is not ideal, but\n",
    "the network should be able to compensate based on whether or not the\n",
    "point lies on an increasing or decreasing region in the encoded space.\n",
    "\n",
    "### Price Adjustment\n",
    "\n",
    "The close and adjusted close price given in the original dataset were used to\n",
    "rescale the other original price metrics (high, low, open). This was\n",
    "accomplished by apply the following transformation to each unscaled\n",
    "price in the record:\n",
    "\n",
    "$$\n",
    "\\text{scaled} = \\text{unscaled} \\cdot\n",
    "\\frac{\\text{adjclose}}{\\text{close}}\n",
    "$$\n",
    "\n",
    "After this point the unscaled prices were discarded. This places all\n",
    "prices on a shared scale that is invariant to stock splits and other\n",
    "price-altering events, improving continuity.\n",
    "\n",
    "### Future Price Calculation\n",
    "\n",
    "A future percent change in closing price was calculated for each\n",
    "record by looking $x$ days into the future and calculating a pointwise\n",
    "or aggregate measure of closing price over those $x$ days. One such\n",
    "strategy is to calculate a pointwise percent change in\n",
    "price as follows, where $i$ indicates an index of the record to be\n",
    "labeled:\n",
    "\n",
    "$$\n",
    "\\Delta = \\frac{P_{i+x} - P_{i}}{P_i} * 100\n",
    "$$\n",
    "\n",
    "Another strategy is to calculate an aggregate of some price metric\n",
    "over $x$ days in the future and calculate percent change from the present\n",
    "price to that aggregate price. For example, one could calculate an average closing\n",
    "price over the following $x$ days and then calculate percent change\n",
    "in closing price from the present to that average.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\Delta = \\frac{a - P_{i}}{P_i} * 100\n",
    "\t&\\text{where}&\n",
    "\t&a = \\frac{1}{x}\\sum_{j=i+1}^{i+x} P_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The decision to use a pointwise or aggregate basis for percent change\n",
    "calculation is dependent on the choice of window size among other\n",
    "things. The choice of window size is primarily determined by the style\n",
    "of trading to be used, along with the interval between records (if\n",
    "this approach were to be applied to by the minute data for day\n",
    "trading). For experimentation, percent change in future price was\n",
    "calculated relative to the average closing price over a future $3$ day\n",
    "window.\n",
    "\n",
    "### Spark ML Pipeline\n",
    "\n",
    "A parameterized Spark ML pipeline is used to extract feature vectors and produce\n",
    "discretized labels for each record. The pipeline consists of the\n",
    "following stages:\n",
    "\n",
    "1. `VectorAssembler` - Collects feature columns into a single vector\n",
    "\tfor processing\n",
    "\n",
    "2. Normalizer - A strategy to normalize each feature to a shared\n",
    "\tdomain. Can be one of the following, or excluded entirely:\n",
    "\t* `StandardScaler` - Rescales each feature to zero mean unit variance\n",
    "\t* `MaxAbsScaler` - Rescales each feature to the range $[0, 1]$,\n",
    "\t\t**preserving sparsity**\n",
    "\n",
    "3. Labeler - A strategy to assign examples a discretized label based\n",
    "\t on future percent change. Can be one of the following, or excluded entirely:\n",
    "\t* `Bucketizer` - Label based on fixed sized buckets of percent\n",
    "\t\tchange\n",
    "\t* `QuantileDiscretizer` - Label based on variable sized buckets such\n",
    "\t\tthat each each class contains an equal number of examples\n",
    "\n",
    "The each record in the output of this pipeline will have a vector of\n",
    "(normalized) features for that day, a future percent change value, and\n",
    "an integer label if requested. By retaining both the future percent\n",
    "change and a discretized label, each record is suitable for use with\n",
    "classification or regression networks.\n",
    "\n",
    "It is important to consider the distribution of examples among the\n",
    "possible classes when using `Bucketizer` as a labeling strategy.\n",
    "It was observed that percent change tends to concentrate near zero,\n",
    "meaning that the bucket that overlaps with zero will have a\n",
    "potentially large number of examples. This is likely to become more of\n",
    "an issue with larger future window sizes, as price will have more time\n",
    "to regress to the mean.\n",
    "\n",
    "Conversely, when using\n",
    "`QuantileDiscretizer` a uniform distribution of examples over each\n",
    "class will be created by compressing or expanding bucket ranges such\n",
    "that the $n$'th bucket contains the top $1/n$'th percent change values.\n",
    "This will manifest as buckets that span only a few percentage points of\n",
    "percent change, taxing the network's ability to distinguish these\n",
    "classes. However, it does\n",
    "eliminate concerns that the network will exhibit a bias towards zero\n",
    "price movement predictions.\n",
    "\n",
    "During experimentation the `MaxAbsScaler` was used as a normalization\n",
    "strategy and the `Bucketizer` was used for a labeling strategy with\n",
    "a bucket parameter list of $-5,-2,2,5$. This produced five discretized\n",
    "classes in the resultant training set. Unequal bucket widths\n",
    "were important in creating a somewhat balanced distribution of\n",
    "examples among the labels while not forcing the network to learn\n",
    "unreasonably small differences between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Examples\n",
    "\n",
    "Having extracted numerically stable features and labels from the\n",
    "original dataset, steps must now be taken to construct complete training\n",
    "examples. Taking inspiration from human styles of trading, the\n",
    "decision was made to create training examples by aggregating\n",
    "chronologically ordered feature vectors over a sliding historical window into a\n",
    "feature matrix.\n",
    "\n",
    "As a concrete example, consider training example $i$ with features\n",
    "$h,l,o,c,v,p$ and a window size of $x$. Example $i$ would then consist\n",
    "of features over the $x$ previous days for that stock as follows:\n",
    "\n",
    "$$\n",
    "\\text{example}_i = \\begin{bmatrix}\n",
    "\th_i & l_i & o_i & c_i & v_i & p_i \\\\\n",
    "\th_{i-1} & l_{i-1} & o_{i-1} & c_{i-1} & v_{i-1} & p_{i-1} \\\\\n",
    "\th_{i-2} & l_{i-2} & o_{i-2} & c_{i-2} & v_{i-2} & p_{i-2} \\\\\n",
    "\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\th_{i-x} & l_{i-x} & o_{i-x} & c_{i-x} & v_{i-x} & p_{i-x} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The result of this process is an intuitively constructed training\n",
    "example, consisting of an ordered time series of features over the $x$\n",
    "previous days and a label indicating price movement in the near\n",
    "future. Implementation of this process is discussed in the following\n",
    "section.\n",
    "\n",
    "We can take further steps to improve the quality of the resultant\n",
    "training examples. Consider the case where the process described above\n",
    "is applied to two adjacent records, $i$ and $i+1$. The result will be\n",
    "two training examples with $x \\times 6$ feature matrices which have a\n",
    "substantial overlap. Specifically, we will produce feature matrices as\n",
    "follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tfeatures_i \\in \\mathbb{R}^{x \\times 6} &=\n",
    "\t\\begin{bmatrix}\n",
    "\t\th_i & l_i & o_i & c_i & v_i & p_i \\\\\n",
    "\t\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\t\th_{i-x} & l_{i-x} & o_{i-x} & c_{i-x} & v_{i-x} & p_{i-x} \\\\\n",
    "\t\\end{bmatrix}\n",
    "\t\\\\\n",
    "\tfeatures_{i+1} \\in \\mathbb{R}^{x \\times 6} &=\n",
    "\t\\begin{bmatrix}\n",
    "\t\th_{i+1} & l_{i+1} & o_{i+1} & c_{i+1} & v_{i+1} & p_{i+1} \\\\\n",
    "\t\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\t\th_{i-x+1} & l_{i-x+1} & o_{i-x+1} & c_{i-x+1} & v_{i-x+1} & p_{i-x+1} \\\\\n",
    "\t\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Applying a stride greater than $1$ to the historical window will\n",
    "reduce the amount of overlap between feature matrices, creating a\n",
    "more diverse training set. For experimentation a stride of $5$ days\n",
    "was chosen. This resulted in an adequate number of training examples\n",
    "and minimized the overlap in the $3$ day future percent change\n",
    "window.\n",
    "\n",
    "Generating the feature matrices as described above is a\n",
    "computationally expensive process, especially at small window\n",
    "strides and large window sizes. To put this in perspective, consider a\n",
    "training set produced from $1000$ stock symbols from $2008$ to $2018$\n",
    "with a window size of $128$ days and a window stride of $1$. Assume\n",
    "the stock only trades $200$ days out of the year.  The\n",
    "number of resultant examples is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tN &= 1000 * 200 * (2018 - 2008) \\\\\n",
    "\t&= 2,000,000\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, if we consider this process applied the entirety of the\n",
    "dataset without constraint\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tN &= 8000 * 200 * (2018 - 1971) \\\\\n",
    "\t&= 75,200,000\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We will have a $128 \\times 6$ feature matrix per row.\n",
    "If we assume that each feature and the percent change is stored as a\n",
    "$32$ bit float and the label is stored as an $8$ bit integer, we can\n",
    "calculated the expected size of the resultant training set in\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tfeatures &= N \\left(128 * 6 * 4\\right) \\\\\n",
    "\tlabels &= N \\left(4 + 1\\right) \\\\\n",
    "\tS &= labels + features \\\\\n",
    "\t&\\approx 230 GB\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
